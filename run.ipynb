{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkzard05/Cora_dataset_benchmark/blob/main/run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U_rypC--3Mc",
        "outputId": "57596095-e837-4ca9-eed8-1377c1db53f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.1 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 30.9 MB/s \n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch_geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=624a57cb951cd88768d9953df1e9026e76914859ae09c84e37373d808dc1620a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4 torch-scatter-2.0.9 torch-sparse-0.6.13 torch-spline-conv-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter torch-sparse torch-spline-conv torch_geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8-smDMP-9e3",
        "outputId": "634dd839-29c0-4376-f74f-ce898ee8c8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import SplineConv\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWyd8fOJCGNb",
        "outputId": "8c5f1d92-453d-4b16-ccc1-81a9a26eaeea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'Cora'\n",
        "dataset = Planetoid(root='/tmp/'+dataset_name, name=dataset_name, \n",
        "                    transform=T.TargetIndegree()\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xBJPAHEcxZ",
        "outputId": "9dacf84d-a677-4c3e-e131-db9d447a3f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset name: Cora\n",
            "number of nodes: 2708 // number of edges: 10556 // number of classes: 7\n",
            "number of node features: 1433 // number of edge features: 1\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
            "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]]) tensor([[0.0179],\n",
            "        [0.0238],\n",
            "        [0.0179],\n",
            "        ...,\n",
            "        [0.1964],\n",
            "        [0.0238],\n",
            "        [0.0238]]) tensor([3, 4, 4,  ..., 3, 3, 3])\n",
            "tensor(140) tensor(500) tensor(1000)\n"
          ]
        }
      ],
      "source": [
        "data = dataset[0]\n",
        "print(f'dataset name: {dataset_name}')\n",
        "print(f'number of nodes: {data.num_nodes} // number of edges: {data.num_edges} // number of classes: {dataset.num_classes}')\n",
        "print(f'number of node features: {data.num_node_features} // number of edge features: {data.num_edge_features}')\n",
        "print(data.x, data.edge_index, data.edge_attr, data.y)\n",
        "print(data.train_mask.sum(), data.val_mask.sum(), data.test_mask.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9bBboRu4q0I",
        "outputId": "10dfa68f-1135-4abf-f6a8-c7be9663a368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda is available\n",
            "appnp(\n",
            "  (layer1): Linear(in_features=1433, out_features=16, bias=True)\n",
            "  (layer2): Linear(in_features=16, out_features=7, bias=True)\n",
            "  (prop): APPNP(K=50, alpha=0.1)\n",
            ")\n",
            "{'layer1.weight': [16, 1433], 'layer1.bias': [16], 'layer2.weight': [7, 16], 'layer2.bias': [7]}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "sgconv(\n",
            "  (layer1): SGConv(1433, 7, K=2)\n",
            ")\n",
            "{'layer1.lin.weight': [7, 1433], 'layer1.lin.bias': [7]}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "splineconv(\n",
            "  (layer1): SplineConv(1433, 16, dim=1)\n",
            "  (layer2): SplineConv(16, 7, dim=1)\n",
            ")\n",
            "{'layer1.weight': [2, 1433, 16], 'layer1.bias': [16], 'layer1.lin.weight': [16, 1433], 'layer2.weight': [2, 16, 7], 'layer2.bias': [7], 'layer2.lin.weight': [7, 16]}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "\n",
        "from model import appnp, sgconv, splineconv\n",
        "\n",
        "print('cuda' if torch.cuda.is_available() else 'cpu', 'is available')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = dataset[0].to(device)\n",
        "hidden, K_sgconv, K_appnp, alpha = 16, 2, 50, 0.1\n",
        "model_list, optimizer_list = [], []\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model_list.append(appnp(dataset=dataset, hidden=hidden, K=K_appnp, alpha=alpha).to(device))\n",
        "model_list.append(sgconv(dataset=dataset, K=K_sgconv).to(device))\n",
        "model_list.append(splineconv(dataset=dataset, hidden=hidden).to(device))\n",
        "\n",
        "for model in model_list:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-3)\n",
        "    optimizer_list.append(optimizer)\n",
        "    model.reset_parameters()\n",
        "    print(model)\n",
        "    print({i:list(j.shape) for i, j in model.named_parameters()})\n",
        "    print('-' * 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss = criterion(model(data)[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def test(model, optimizer, mask):\n",
        "    model.eval()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    correct = model(data).argmax(dim=1)[mask] == data.y[mask]\n",
        "    return int(correct.sum()) / int(mask.sum())\n",
        "\n",
        "result = {}\n",
        "\n",
        "for model, optimizer in zip(model_list, optimizer_list):\n",
        "    best_acc = 0\n",
        "    epochs = 1000\n",
        "    for epoch in range(1, epochs+1):\n",
        "        loss = train(model, optimizer)\n",
        "        train_acc = test(model, optimizer, data.train_mask)\n",
        "        val_acc = test(model, optimizer, data.val_mask)\n",
        "        test_acc = test(model, optimizer, data.test_mask)\n",
        "        if best_acc < test_acc:\n",
        "            best_acc = test_acc\n",
        "            print(f'epoch: {epoch} | loss: {loss.item():.4f} | train_acc: {train_acc:.4f} | val_acc: {val_acc} | test_acc: {test_acc}')\n",
        "    result[model] = best_acc\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgV-kN1uulI",
        "outputId": "bb819439-e80d-4584-a3e7-acba4da2dc88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 | loss: 1.9461 | train_acc: 0.1429 | val_acc: 0.058 | test_acc: 0.065\n",
            "epoch: 2 | loss: 1.9241 | train_acc: 0.4071 | val_acc: 0.254 | test_acc: 0.248\n",
            "epoch: 3 | loss: 1.8997 | train_acc: 0.6000 | val_acc: 0.392 | test_acc: 0.43\n",
            "epoch: 4 | loss: 1.8636 | train_acc: 0.6500 | val_acc: 0.49 | test_acc: 0.525\n",
            "epoch: 5 | loss: 1.8280 | train_acc: 0.6714 | val_acc: 0.522 | test_acc: 0.561\n",
            "epoch: 6 | loss: 1.7877 | train_acc: 0.6714 | val_acc: 0.532 | test_acc: 0.582\n",
            "epoch: 7 | loss: 1.7517 | train_acc: 0.6714 | val_acc: 0.548 | test_acc: 0.591\n",
            "epoch: 8 | loss: 1.6972 | train_acc: 0.6857 | val_acc: 0.554 | test_acc: 0.596\n",
            "epoch: 9 | loss: 1.6529 | train_acc: 0.6857 | val_acc: 0.556 | test_acc: 0.597\n",
            "epoch: 14 | loss: 1.4616 | train_acc: 0.6857 | val_acc: 0.558 | test_acc: 0.6\n",
            "epoch: 15 | loss: 1.4301 | train_acc: 0.6857 | val_acc: 0.562 | test_acc: 0.602\n",
            "epoch: 17 | loss: 1.3579 | train_acc: 0.6929 | val_acc: 0.562 | test_acc: 0.605\n",
            "epoch: 18 | loss: 1.3026 | train_acc: 0.6929 | val_acc: 0.566 | test_acc: 0.606\n",
            "epoch: 19 | loss: 1.3006 | train_acc: 0.6929 | val_acc: 0.566 | test_acc: 0.609\n",
            "epoch: 20 | loss: 1.2595 | train_acc: 0.6929 | val_acc: 0.572 | test_acc: 0.611\n",
            "epoch: 21 | loss: 1.2373 | train_acc: 0.6929 | val_acc: 0.572 | test_acc: 0.612\n",
            "epoch: 23 | loss: 1.1880 | train_acc: 0.7000 | val_acc: 0.57 | test_acc: 0.614\n",
            "epoch: 24 | loss: 1.1485 | train_acc: 0.7000 | val_acc: 0.572 | test_acc: 0.623\n",
            "epoch: 25 | loss: 1.1735 | train_acc: 0.7000 | val_acc: 0.574 | test_acc: 0.626\n",
            "epoch: 27 | loss: 1.1226 | train_acc: 0.7071 | val_acc: 0.574 | test_acc: 0.628\n",
            "epoch: 36 | loss: 1.0328 | train_acc: 0.7071 | val_acc: 0.57 | test_acc: 0.63\n",
            "epoch: 38 | loss: 0.9982 | train_acc: 0.7071 | val_acc: 0.57 | test_acc: 0.631\n",
            "epoch: 47 | loss: 0.9762 | train_acc: 0.7071 | val_acc: 0.576 | test_acc: 0.633\n",
            "epoch: 60 | loss: 0.9456 | train_acc: 0.7357 | val_acc: 0.584 | test_acc: 0.652\n",
            "epoch: 61 | loss: 0.9402 | train_acc: 0.7643 | val_acc: 0.6 | test_acc: 0.667\n",
            "epoch: 62 | loss: 0.9001 | train_acc: 0.7857 | val_acc: 0.612 | test_acc: 0.685\n",
            "epoch: 63 | loss: 0.9301 | train_acc: 0.8071 | val_acc: 0.614 | test_acc: 0.695\n",
            "epoch: 64 | loss: 0.9530 | train_acc: 0.8143 | val_acc: 0.628 | test_acc: 0.699\n",
            "epoch: 65 | loss: 0.9093 | train_acc: 0.8214 | val_acc: 0.64 | test_acc: 0.707\n",
            "epoch: 66 | loss: 0.8958 | train_acc: 0.8286 | val_acc: 0.646 | test_acc: 0.718\n",
            "epoch: 67 | loss: 0.8864 | train_acc: 0.8357 | val_acc: 0.662 | test_acc: 0.719\n",
            "epoch: 69 | loss: 0.8696 | train_acc: 0.8429 | val_acc: 0.666 | test_acc: 0.723\n",
            "epoch: 70 | loss: 0.8566 | train_acc: 0.8429 | val_acc: 0.678 | test_acc: 0.73\n",
            "epoch: 71 | loss: 0.8182 | train_acc: 0.8500 | val_acc: 0.678 | test_acc: 0.734\n",
            "epoch: 72 | loss: 0.8171 | train_acc: 0.8500 | val_acc: 0.682 | test_acc: 0.743\n",
            "epoch: 73 | loss: 0.7864 | train_acc: 0.8500 | val_acc: 0.684 | test_acc: 0.745\n",
            "epoch: 74 | loss: 0.7539 | train_acc: 0.8643 | val_acc: 0.694 | test_acc: 0.75\n",
            "epoch: 75 | loss: 0.7367 | train_acc: 0.8857 | val_acc: 0.702 | test_acc: 0.761\n",
            "epoch: 85 | loss: 0.5175 | train_acc: 0.9429 | val_acc: 0.726 | test_acc: 0.771\n",
            "epoch: 86 | loss: 0.5326 | train_acc: 0.9786 | val_acc: 0.754 | test_acc: 0.793\n",
            "epoch: 87 | loss: 0.4755 | train_acc: 0.9929 | val_acc: 0.762 | test_acc: 0.795\n",
            "epoch: 94 | loss: 0.3696 | train_acc: 1.0000 | val_acc: 0.774 | test_acc: 0.797\n",
            "epoch: 95 | loss: 0.4113 | train_acc: 1.0000 | val_acc: 0.78 | test_acc: 0.801\n",
            "epoch: 96 | loss: 0.3809 | train_acc: 1.0000 | val_acc: 0.788 | test_acc: 0.812\n",
            "epoch: 106 | loss: 0.3010 | train_acc: 1.0000 | val_acc: 0.788 | test_acc: 0.813\n",
            "epoch: 109 | loss: 0.2999 | train_acc: 1.0000 | val_acc: 0.792 | test_acc: 0.826\n",
            "epoch: 110 | loss: 0.2924 | train_acc: 0.9929 | val_acc: 0.792 | test_acc: 0.833\n",
            "epoch: 126 | loss: 0.2518 | train_acc: 1.0000 | val_acc: 0.802 | test_acc: 0.836\n",
            "epoch: 127 | loss: 0.2584 | train_acc: 1.0000 | val_acc: 0.804 | test_acc: 0.837\n",
            "epoch: 163 | loss: 0.2502 | train_acc: 1.0000 | val_acc: 0.802 | test_acc: 0.839\n",
            "epoch: 187 | loss: 0.2285 | train_acc: 1.0000 | val_acc: 0.806 | test_acc: 0.842\n",
            "epoch: 253 | loss: 0.2447 | train_acc: 1.0000 | val_acc: 0.814 | test_acc: 0.846\n",
            "epoch: 254 | loss: 0.1835 | train_acc: 1.0000 | val_acc: 0.818 | test_acc: 0.848\n",
            "epoch: 597 | loss: 0.1819 | train_acc: 1.0000 | val_acc: 0.81 | test_acc: 0.853\n",
            "epoch: 598 | loss: 0.2072 | train_acc: 0.9929 | val_acc: 0.814 | test_acc: 0.855\n",
            "epoch: 1 | loss: 1.9503 | train_acc: 0.6143 | val_acc: 0.474 | test_acc: 0.5\n",
            "epoch: 2 | loss: 1.9021 | train_acc: 0.7357 | val_acc: 0.574 | test_acc: 0.613\n",
            "epoch: 3 | loss: 1.8340 | train_acc: 0.8000 | val_acc: 0.632 | test_acc: 0.658\n",
            "epoch: 4 | loss: 1.7631 | train_acc: 0.8286 | val_acc: 0.646 | test_acc: 0.681\n",
            "epoch: 5 | loss: 1.6993 | train_acc: 0.8286 | val_acc: 0.668 | test_acc: 0.69\n",
            "epoch: 6 | loss: 1.6264 | train_acc: 0.8500 | val_acc: 0.68 | test_acc: 0.696\n",
            "epoch: 7 | loss: 1.5736 | train_acc: 0.8857 | val_acc: 0.702 | test_acc: 0.715\n",
            "epoch: 8 | loss: 1.4997 | train_acc: 0.9286 | val_acc: 0.716 | test_acc: 0.732\n",
            "epoch: 9 | loss: 1.4287 | train_acc: 0.9357 | val_acc: 0.72 | test_acc: 0.744\n",
            "epoch: 10 | loss: 1.3534 | train_acc: 0.9429 | val_acc: 0.732 | test_acc: 0.754\n",
            "epoch: 11 | loss: 1.2998 | train_acc: 0.9429 | val_acc: 0.74 | test_acc: 0.765\n",
            "epoch: 12 | loss: 1.2485 | train_acc: 0.9571 | val_acc: 0.744 | test_acc: 0.769\n",
            "epoch: 13 | loss: 1.1822 | train_acc: 0.9714 | val_acc: 0.744 | test_acc: 0.772\n",
            "epoch: 14 | loss: 1.1156 | train_acc: 0.9714 | val_acc: 0.74 | test_acc: 0.779\n",
            "epoch: 15 | loss: 1.0808 | train_acc: 0.9786 | val_acc: 0.744 | test_acc: 0.783\n",
            "epoch: 18 | loss: 0.9133 | train_acc: 0.9857 | val_acc: 0.748 | test_acc: 0.787\n",
            "epoch: 19 | loss: 0.8915 | train_acc: 0.9857 | val_acc: 0.752 | test_acc: 0.79\n",
            "epoch: 20 | loss: 0.8614 | train_acc: 0.9857 | val_acc: 0.754 | test_acc: 0.792\n",
            "epoch: 21 | loss: 0.8316 | train_acc: 0.9857 | val_acc: 0.762 | test_acc: 0.795\n",
            "epoch: 22 | loss: 0.7918 | train_acc: 0.9857 | val_acc: 0.764 | test_acc: 0.796\n",
            "epoch: 23 | loss: 0.7673 | train_acc: 0.9857 | val_acc: 0.766 | test_acc: 0.8\n",
            "epoch: 24 | loss: 0.7375 | train_acc: 0.9857 | val_acc: 0.766 | test_acc: 0.802\n",
            "epoch: 25 | loss: 0.7145 | train_acc: 0.9857 | val_acc: 0.766 | test_acc: 0.803\n",
            "epoch: 30 | loss: 0.6110 | train_acc: 0.9857 | val_acc: 0.77 | test_acc: 0.804\n",
            "epoch: 32 | loss: 0.5766 | train_acc: 0.9857 | val_acc: 0.774 | test_acc: 0.806\n",
            "epoch: 33 | loss: 0.5782 | train_acc: 0.9857 | val_acc: 0.774 | test_acc: 0.809\n",
            "epoch: 35 | loss: 0.5437 | train_acc: 0.9857 | val_acc: 0.776 | test_acc: 0.81\n",
            "epoch: 37 | loss: 0.5306 | train_acc: 0.9857 | val_acc: 0.778 | test_acc: 0.812\n",
            "epoch: 41 | loss: 0.4945 | train_acc: 0.9857 | val_acc: 0.784 | test_acc: 0.813\n",
            "epoch: 46 | loss: 0.4718 | train_acc: 0.9857 | val_acc: 0.794 | test_acc: 0.814\n",
            "epoch: 47 | loss: 0.4785 | train_acc: 0.9857 | val_acc: 0.796 | test_acc: 0.816\n",
            "epoch: 61 | loss: 0.4170 | train_acc: 0.9857 | val_acc: 0.794 | test_acc: 0.817\n",
            "epoch: 78 | loss: 0.4278 | train_acc: 0.9857 | val_acc: 0.79 | test_acc: 0.818\n",
            "epoch: 89 | loss: 0.4003 | train_acc: 0.9857 | val_acc: 0.79 | test_acc: 0.819\n",
            "epoch: 90 | loss: 0.4004 | train_acc: 0.9857 | val_acc: 0.79 | test_acc: 0.82\n",
            "epoch: 95 | loss: 0.3963 | train_acc: 0.9929 | val_acc: 0.786 | test_acc: 0.821\n",
            "epoch: 98 | loss: 0.3923 | train_acc: 0.9929 | val_acc: 0.788 | test_acc: 0.822\n",
            "epoch: 139 | loss: 0.3991 | train_acc: 0.9929 | val_acc: 0.78 | test_acc: 0.823\n",
            "epoch: 140 | loss: 0.3832 | train_acc: 0.9929 | val_acc: 0.778 | test_acc: 0.824\n",
            "epoch: 142 | loss: 0.3964 | train_acc: 0.9929 | val_acc: 0.778 | test_acc: 0.825\n",
            "epoch: 148 | loss: 0.3839 | train_acc: 0.9929 | val_acc: 0.772 | test_acc: 0.826\n",
            "epoch: 170 | loss: 0.3708 | train_acc: 0.9929 | val_acc: 0.772 | test_acc: 0.827\n",
            "epoch: 174 | loss: 0.3794 | train_acc: 0.9929 | val_acc: 0.77 | test_acc: 0.828\n",
            "epoch: 213 | loss: 0.3699 | train_acc: 0.9929 | val_acc: 0.78 | test_acc: 0.829\n",
            "epoch: 216 | loss: 0.3736 | train_acc: 0.9929 | val_acc: 0.778 | test_acc: 0.83\n",
            "epoch: 464 | loss: 0.3612 | train_acc: 0.9929 | val_acc: 0.784 | test_acc: 0.831\n",
            "epoch: 662 | loss: 0.3531 | train_acc: 0.9929 | val_acc: 0.786 | test_acc: 0.832\n",
            "epoch: 972 | loss: 0.3562 | train_acc: 0.9929 | val_acc: 0.788 | test_acc: 0.834\n",
            "epoch: 974 | loss: 0.3877 | train_acc: 0.9929 | val_acc: 0.788 | test_acc: 0.835\n",
            "epoch: 1 | loss: 1.9451 | train_acc: 0.4500 | val_acc: 0.342 | test_acc: 0.36\n",
            "epoch: 2 | loss: 1.9152 | train_acc: 0.6357 | val_acc: 0.424 | test_acc: 0.45\n",
            "epoch: 3 | loss: 1.8459 | train_acc: 0.7786 | val_acc: 0.456 | test_acc: 0.489\n",
            "epoch: 4 | loss: 1.7747 | train_acc: 0.8286 | val_acc: 0.476 | test_acc: 0.533\n",
            "epoch: 5 | loss: 1.6734 | train_acc: 0.8786 | val_acc: 0.532 | test_acc: 0.583\n",
            "epoch: 6 | loss: 1.6074 | train_acc: 0.9071 | val_acc: 0.57 | test_acc: 0.613\n",
            "epoch: 7 | loss: 1.4781 | train_acc: 0.9429 | val_acc: 0.602 | test_acc: 0.648\n",
            "epoch: 8 | loss: 1.3463 | train_acc: 0.9643 | val_acc: 0.63 | test_acc: 0.68\n",
            "epoch: 9 | loss: 1.2637 | train_acc: 0.9643 | val_acc: 0.652 | test_acc: 0.701\n",
            "epoch: 10 | loss: 1.1582 | train_acc: 0.9786 | val_acc: 0.668 | test_acc: 0.717\n",
            "epoch: 11 | loss: 1.0387 | train_acc: 0.9857 | val_acc: 0.698 | test_acc: 0.735\n",
            "epoch: 12 | loss: 0.9040 | train_acc: 0.9929 | val_acc: 0.7 | test_acc: 0.743\n",
            "epoch: 13 | loss: 0.8775 | train_acc: 0.9929 | val_acc: 0.728 | test_acc: 0.76\n",
            "epoch: 14 | loss: 0.7182 | train_acc: 0.9929 | val_acc: 0.742 | test_acc: 0.774\n",
            "epoch: 15 | loss: 0.6605 | train_acc: 1.0000 | val_acc: 0.752 | test_acc: 0.786\n",
            "epoch: 16 | loss: 0.6041 | train_acc: 1.0000 | val_acc: 0.77 | test_acc: 0.795\n",
            "epoch: 17 | loss: 0.5068 | train_acc: 1.0000 | val_acc: 0.772 | test_acc: 0.8\n",
            "epoch: 18 | loss: 0.5231 | train_acc: 1.0000 | val_acc: 0.78 | test_acc: 0.803\n",
            "epoch: 19 | loss: 0.3962 | train_acc: 1.0000 | val_acc: 0.79 | test_acc: 0.808\n",
            "epoch: 30 | loss: 0.1612 | train_acc: 1.0000 | val_acc: 0.778 | test_acc: 0.811\n",
            "epoch: 34 | loss: 0.1584 | train_acc: 1.0000 | val_acc: 0.78 | test_acc: 0.818\n",
            "epoch: 35 | loss: 0.1525 | train_acc: 1.0000 | val_acc: 0.782 | test_acc: 0.819\n",
            "epoch: 37 | loss: 0.1287 | train_acc: 1.0000 | val_acc: 0.78 | test_acc: 0.82\n",
            "epoch: 38 | loss: 0.1081 | train_acc: 1.0000 | val_acc: 0.778 | test_acc: 0.822\n",
            "epoch: 42 | loss: 0.1535 | train_acc: 1.0000 | val_acc: 0.776 | test_acc: 0.824\n",
            "epoch: 140 | loss: 0.1082 | train_acc: 1.0000 | val_acc: 0.802 | test_acc: 0.826\n",
            "epoch: 158 | loss: 0.0707 | train_acc: 1.0000 | val_acc: 0.79 | test_acc: 0.827\n",
            "epoch: 159 | loss: 0.1172 | train_acc: 1.0000 | val_acc: 0.786 | test_acc: 0.828\n",
            "{appnp(\n",
            "  (layer1): Linear(in_features=1433, out_features=16, bias=True)\n",
            "  (layer2): Linear(in_features=16, out_features=7, bias=True)\n",
            "  (prop): APPNP(K=50, alpha=0.1)\n",
            "): 0.855, sgconv(\n",
            "  (layer1): SGConv(1433, 7, K=2)\n",
            "): 0.835, splineconv(\n",
            "  (layer1): SplineConv(1433, 16, dim=1)\n",
            "  (layer2): SplineConv(16, 7, dim=1)\n",
            "): 0.828}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "run.ipynb",
      "provenance": [],
      "mount_file_id": "1efLPrzJ1VqoyrelimlWfd7HeNdjs3kve",
      "authorship_tag": "ABX9TyN076nlBBEEmDwpEazkOtd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}